{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc65724-2a73-4903-b029-1801b2ff3777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from datasets) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4300a001-a67d-4180-87ca-f20267d2ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245888cb-6d81-4476-b574-18eb7517e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\") # getting this off huggingface\n",
    "\n",
    "split_ratio = 0.8\n",
    "train_size = int(len(dataset['train']) * split_ratio) # test eval splir\n",
    "\n",
    "train_data = dataset['train'].select(range(train_size))  #train data\n",
    "eval_data = dataset['train'].select(range(train_size, len(dataset['train']))) # evaluation data  \n",
    "\n",
    "def write_jsonl(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for row in data:\n",
    "            json_record = {\"act\": row[\"act\"], \"prompt\": row[\"prompt\"]} # making of jsonl for llms\n",
    "            f.write(json.dumps(json_record) + '\\n')\n",
    "\n",
    "write_jsonl(train_data, 'train.jsonl')\n",
    "write_jsonl(eval_data, 'eval.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1a2af8-a5a6-4f7e-bdcd-216c732de146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nesecarry packages\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4395b01e-ff81-4b4d-86a7-af6f0c1362af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fa2823-a0cc-48e7-ad9e-6801ca97e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea51a8e-6506-4f31-a215-5046183e5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d611e74f-4c66-4fbc-a9c5-7b238e32f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets scipy ipywidgets matplotlib einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc63f14f-69bb-4cb6-aefc-558f81307abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminimalistnewsalerts\u001b[0m (\u001b[33mminimalistnewsalerts-sri-sri-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02194706-49ad-4a7e-8314-50cebcc50e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691c9ad55b834c719d790b662ae5384c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd68460514a467bb95b835f05b6d164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='eval.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f05f5-6b0c-4d4e-b817-c98bc96c99cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad563a019c947d7af539d59682d827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "local_dir = \"./microsoftphi2-better\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, cache_dir=local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, cache_dir=local_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509a043-30d5-4cf3-9bf4-ebeefda71d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Act: {example['act']}\\n### Prompt: {example['prompt']}\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a93f27-3b2d-4400-a56c-0761ba0b9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the dataset before tokenization\n",
    "print(train_dataset[0])  # Print a single row from the dataset\n",
    "\n",
    "# Check the first batch to ensure it's structured as expected\n",
    "print(train_dataset[:5])  # Print the first 5 rows as a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb5e59-8aa6-4162-aad6-1bf3ee48ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    max_length =256\n",
    "    formatted_prompts = [formatting_func({\"act\": \"example_act\", \"prompt\": prompt}) for prompt in prompts]\n",
    "    # payload management\n",
    "    result = tokenizer(\n",
    "        formatted_prompts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f0300-6187-4ead-b464-62b1e4d123c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt, batched=True)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2019ba-8662-4cf9-9619-aa3be1c82fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset) # the length is fixed due to hardware constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a2c39-e1ae-4a40-a546-b9cc389fc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8975f0-0bed-4066-9623-a52cf3e51075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "eval_prompt  = \"This is a speech by a CEO of a fintech company\"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False, \n",
    ")\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d836d-5ee1-40e1-8fce-55e6819b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA dimension analysis by returning the trainable params\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "print_trainable_parameters(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e01e8c-9e8c-4dde-9254-4b1f61ea1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827e293-3e81-4551-8746-2927d6dcd83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Making this device and platform agnostic\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"Using CPU\")\n",
    "accelerator = Accelerator()\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"Wqkv\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = accelerator.prepare_model(model)\n",
    "def print_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85b081fb-b0b9-47c1-b7fa-2b290abeca67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./venv/lib/python3.10/site-packages (0.35.0.dev0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./venv/lib/python3.10/site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./venv/lib/python3.10/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.10/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
      "Requirement already satisfied: triton==3.0.0 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b475b29c-4df6-42e7-b670-4797c8fd9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: #making it usable for GPU cloud instance\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95664bd3-5f60-4d84-861b-500db29190f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36ba2d-58f0-4027-bb8d-c751a14d53c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishrut/projs/tensorgo-proj/venv/lib/python3.10/site-packages/transformers/training_args.py:1541: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87efecd25ee048ba9fb65ca5574acce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112953833331732, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vishrut/projs/tensorgo-proj/wandb/run-20240912_141529-a6sw7itp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune/runs/a6sw7itp' target=\"_blank\">phi2-tensorgo-challenge-2024-09-12-14-15</a></strong> to <a href='https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune' target=\"_blank\">https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune/runs/a6sw7itp' target=\"_blank\">https://wandb.ai/minimalistnewsalerts-sri-sri-university/journal-finetune/runs/a6sw7itp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"tensorgo-challenge\"\n",
    "base_model_name = \"phi2\"\n",
    "run_name = f\"{base_model_name}-{project}\"\n",
    "output_dir = f\"./{run_name}\"\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,  # reduce batch size for hardware constraints\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=100,  # reduced step size because of hardware constraints\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=10,  \n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"no\", \n",
    "        evaluation_strategy=\"no\",  \n",
    "        do_eval=False,  \n",
    "        report_to=None,  \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058ccf-f632-4e41-8129-1c133d8f82c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
